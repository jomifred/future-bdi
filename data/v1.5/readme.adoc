= Results of Future-BDI experiments -- Dynamic Environment, v1.5


(see slides with the basics for the dynamic environment xref:../../doc/future-bdi-notes-5.pdf[here])

The _environment_ is the same grid as in previous versions, but now  walls can appear and disappear. The agent has to cross the grid, from line 0 to line 19, as shown below.

image:figs/d-env.png[]

The agent *behaviour* is:

1. Perceive the environment and select a plan based on its policy (a plan implies an action). The policy is: select a plan with a valid action that brings the agent near its destination.
2. Use the matrix to compute the future; if ok, proceed with the selected plan.
3. If not ok, use a recovery strategy to search for an alternative plan (the recovery plan). Then use the action of this plan. [assuming that _recovery plan_ was explained somewhere else, it may be another option now or in the future.]
4. Go to step 1.

We *measure*:

- efficacy. We measure whether the agent _successfully_ cross the grid before a timeout;
- efficiency. We measure the _number of actions_ the agent does to achieve its goal; and
- the cost of that efficiency. We considered to measure elapse time, number of created matrices, and number of visited states. All three are very correlated, so the number of _visited states_ is used.

We *vary*:

- the environment dynamicity (_p_), from environments that do not change (_p_=0) to environments that change every cycle (_p_=1). Changes are implemented by adding or removing walls in the scenario.

- how far the agent looks ahead in the future. It is based on a required certainty (_rc_) of a future state of the matrix (computed from the stochastic properties of the environment). If the matrix reaches a state with less than the required certainty, the agent stops the matrix. For instance, if _rc_=1, the agent only considers future states that it can be fully sure about. If _rc_=0, the agent does not care about certainty.

- the agent recovery strategy (_s_) while searching for a recovery plan when its current policy select plans that bring it to a failure. _s_ = solve_P (other options now), solve_M, solve_F (other options latter), or random. Random strategy consists of selecting a random valid plan in place of the selected plan.

*Terminology*:

- an _episode_ starts with the agent at (15,0) and ends when either it arrives at (15,19) or the deadline is reached.

- _configuration_ : a combination of _p_, _rc_, _s_  used in the execution of an episode.

- environment _cycle_ starts with an environment state, the agent executes an action, and we have the next state (for the next cycle).

== Results

It follows the result of an agent using _s_ = solve_M and _rc_ = 0.9, _p_ varies from 0 to 1 (x-axis). Number of walls is 5. 895 episodes. Executions that took more than 10 seconds were aborted and the agent goal is considered not achieved. (to read the efficacy in the graph, the y line for 100000 visited states corresponds to 100% of efficacy.)

The objective is to evaluate _rc_ in the search for recovery plans. (it is not the use of matrix to detect future failures.)

image:graphs/solve-m-states-0.9.pdf.png[]


Notes:

- The best efficiency (less actions to achieve the goal) is obtained when the environment is not so dynamic (_p_ < 0.1), since the agent can find a good recovery plan that remains good during the episode. It pays for that, of course. In this configuration, a several states are visited in the search for that recovery plan. *we pay for efficiency*. That cost may be so high that the agent misses the deadline (lower efficacy).

- With _p_ > 0.5, the agent has not enough certainty about the future of its environment to properly search for recovery plans and thus few states are visited (low cost). When its policy can not be applied (the selected plan leads to a bad future) and no recovery plans can be found (for lack of certainty), the agent simply acts randomly. In case it gets stuck, either the wall blocking it disappear or the random movements will place it in a state where its policy can be applied again, hopefully achieving its goal (so the high efficacy of this case). *no cost, no gain*, *random scenario -> random strategy*

- Around _p_~0.2 we have the worst performance, both in terms of efficiency and cost, also not great in terms of efficacy. Given this dynamicity, the agent can still dedicate time in the search for recovery plans. Using these plans, the agent moves out of its policy to avoid walls, but these walls will likely disappear (specially walls that are far from the agent). To deviate produces unnecessary steps (thus the low efficiency). The recovery plans (expensive to find) might be inefficient in the future when the walls may not be there anymore or others are added. *waist of energy (computational and actions) for an uncertainty environment*

- The efficiency of random strategy gets better as the environment becomes more dynamic. With _p_ > 0.3, its efficiency is almost the same as solve_M. Notice that there is no cost in this strategy.

//- With _p_=0 we do not have the best efficiency! The reason is the solve_f strategy, that avoids to deviate from the agent policy (that is not optimal in these experiments). When _p_ increases a bit, recovery plans tend to select more efficient plans.

- For _rc_~0.9, better results can be achieved in an environment that is either more static or highly dynamic. It is particularly difficult to be effective/efficient/cheap in environment with _p_ ~ 0.2. (this _p_ value changes as we change _rc_ as seen in the next Section.)


- Conclusion, in configurations (., 0.9, solve_M), it is useful to search for recovery plans until _p_ < 0.2, after that, it is better to select actions randomly when the policy has no future.

== Choosing _rc_

It follows the result of an agent using _s_ = solve_M. The environment dynamicity (_p_) varies from 0 to 1 (x-axis). Number of walls is 5. 4737 episodes. Executions that took more than 10 seconds were aborted and the agent goal is considered not achieved.

The objective is to evaluate different required certainties (_rc_) on different environments, with different dynamicity (_p_).

The following graph shows the behaviour of efficacy (percentage of executions where the agent has crossed the grid before 10 seconds). *As _rc_ decrease, the efficacy decrease*.

image:graphs/solve-m-a-success.pdf.png[]

The following graph shows the behaviour of efficiency (how many actions to cross the grid). *As _rc_ decrease, the efficiency increase*.

image:graphs/solve-m-a-eff.pdf.png[]

The following graph shows the behaviour of cost (how many states were visited to select recovery choices) . *As _rc_ decrease, the cost increase*.

image:graphs/solve-m-a-cost.pdf.png[]

Notes about choosing _rc_:

- high dynamic environment (_p_ ~ 0.9): *high _rc_ is preferable*.

* all _rc_ have equal efficiency.

* high _rc_ has better cost and efficacy. Notice that with _rc_=1 and _p_=1, we have the same case of the random strategy, since no matrix can run in that configuration.

- low dynamic environment (_p_ ~ 0.1):

* if efficiency is not an issue, high _rc_ is preferable, given the low cost and high efficacy.

* if efficiency is an issue, _rc_ ~ 0.9 has a good balance (not so much cost, not so bad efficacy)

//- medium dynamic environment (_p_ ~ 0.5): really depends on what is more important for the application. In general, _rc_ ~ 0.5 has a good balance among the three criteria.

- as _rc_ increase, the _p_ of the worst case also increase.

- of course, low _rc_ does not make sense, reasoning about the uncertain! Its efficiency is quite good: the few times the agent succeed to cross the grid, it did it quite fast. This result is unlikely (see efficacy).
