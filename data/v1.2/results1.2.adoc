= Results of Future-BDI experiments -- Single Agent, v1.2

Scenario line:: little "detour".

image:../v1.1/screens1.1/line-m.png[]

Scenario U:: good for the agent policy, it needs to anticipate that entering inside the U, which is not a good option.

image:../v1.1/screens1.1/u-m.png[]

Scenario H:: bad for the policy

image:../v1.1/screens1.1/h-m.png[]

Scenario O:: no solution

image:../v1.1/screens1.1/o-o.png[]


[cols="1,1,>1,>1,>1,>1,>1"]
|===
|scenario | strategy | solve | matrices | visited | steps | steps in policy

| line | NONE | no | 0 | &infin; | &infin; | &infin; (100%)
include::stats1.2.txt[]

|===

*Columns*

- matrices: number of explored matrices
- visited: number of states mentally or concretely visited
- steps: steps performed (behavior after reasoning)
- in policy: among the steps, how many follows the policy  preference

*Remarks*:

* ONE: does not solve, as NONE, but does not spend energy for the goal. Discovers it in *linear time*!
* SOLVE_F stays more in policy in all scenarios.
* SOLVE_M stays in policy more than SOLVE_P, with the same number of steps (efficiency).  slower in  some scenarios, faster in  others. middle term strategy (of course)

* scenario line, SOLVE_F is faster, the agent policy works well here.
* scenario U, solve_P is faster, as soon as the bad option is taken, the better
* scenario H, does not add anything significant result. all strategies are equal. policy is not good in this scenario.
* scenario O, ONE is the best strategy in case of no solution in the agent policy.

